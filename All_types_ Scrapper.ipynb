{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79353e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Divar car images scraper...\n",
      "\n",
      "Starting scraping for Ù¾Ú˜Ùˆ206...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Ù¾Ú˜Ùˆ206 pages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images for Ù¾Ú˜Ùˆ206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Ù¾Ú˜Ùˆ206: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [02:43<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting scraping for Ù¾Ú˜Ùˆ207...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Ù¾Ú˜Ùˆ207 pages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images for Ù¾Ú˜Ùˆ207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Ù¾Ú˜Ùˆ207: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [02:43<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting scraping for Ø³Ù…Ù†Ø¯...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Ø³Ù…Ù†Ø¯ pages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images for Ø³Ù…Ù†Ø¯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Ø³Ù…Ù†Ø¯: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [02:42<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting scraping for Ø¯Ù†Ø§...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Ø¯Ù†Ø§ pages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images for Ø¯Ù†Ø§\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Ø¯Ù†Ø§: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [02:41<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting scraping for ØªØ§Ø±Ø§...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping ØªØ§Ø±Ø§ pages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69 images for ØªØ§Ø±Ø§\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading ØªØ§Ø±Ø§: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [02:44<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting scraping for Ø±Ø§Ù†Ø§...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Ø±Ø§Ù†Ø§ pages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images for Ø±Ø§Ù†Ø§\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Ø±Ø§Ù†Ø§: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [02:40<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Download Summary:\n",
      "- Ù¾Ú˜Ùˆ206: 72 images\n",
      "- Ù¾Ú˜Ùˆ207: 72 images\n",
      "- Ø³Ù…Ù†Ø¯: 72 images\n",
      "- Ø¯Ù†Ø§: 72 images\n",
      "- ØªØ§Ø±Ø§: 69 images\n",
      "- Ø±Ø§Ù†Ø§: 72 images\n",
      "\n",
      "âœ… All operations completed! Total images downloaded: 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Referer\": \"https://divar.ir/\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\"\n",
    "}\n",
    "\n",
    "# Target models with search queries (URL encoded)\n",
    "target_models = {\n",
    "    \"Ù¾Ú˜Ùˆ206\": \"Ù¾Ú˜Ùˆ%20206\",\n",
    "    \"Ù¾Ú˜Ùˆ207\": \"Ù¾Ú˜Ùˆ%20207\",\n",
    "    \"Ø³Ù…Ù†Ø¯\": \"Ø³Ù…Ù†Ø¯\",\n",
    "    \"Ø¯Ù†Ø§\": \"Ø¯Ù†Ø§\",\n",
    "    \"ØªØ§Ø±Ø§\": \"ØªØ§Ø±Ø§\",\n",
    "    \"Ø±Ø§Ù†Ø§\": \"Ø±Ø§Ù†Ø§\",\n",
    "}\n",
    "\n",
    "BASE_URL = \"https://divar.ir/s/iran/car?q={query}\"\n",
    "NUM_PAGES_TO_SCRAPE = 10\n",
    "MAX_IMAGES_PER_MODEL = 200\n",
    "DOWNLOAD_DELAY = (1, 3)  # Random delay between downloads in seconds\n",
    "\n",
    "def create_folders():\n",
    "    \"\"\"Create folders for each car model\"\"\"\n",
    "    base_folder = \"Cars\"\n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "    for model in target_models:\n",
    "        os.makedirs(os.path.join(base_folder, model), exist_ok=True)\n",
    "    return base_folder\n",
    "\n",
    "def get_image_urls(page_url):\n",
    "    \"\"\"Extract image URLs from a Divar page\"\"\"\n",
    "    try:\n",
    "        response = requests.get(page_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        image_urls = []\n",
    "        image_tags = soup.select('img.kt-image-block__image')\n",
    "        \n",
    "        for img in image_tags:\n",
    "            img_url = img.get('data-src') or img.get('src')\n",
    "            if img_url and img_url.startswith('https://s100.divarcdn.com/static/photo'):\n",
    "                image_urls.append(img_url)\n",
    "        \n",
    "        return list(set(image_urls))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting image URLs from {page_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_model(model_name, query, base_folder):\n",
    "    \"\"\"Scrape images for a specific car model\"\"\"\n",
    "    print(f\"\\nStarting scraping for {model_name}...\")\n",
    "    all_image_urls = []\n",
    "    \n",
    "    # Scrape multiple pages\n",
    "    for page_num in tqdm(range(1, NUM_PAGES_TO_SCRAPE + 1), desc=f\"Scraping {model_name} pages\"):\n",
    "        page_url = BASE_URL.format(query=query) + f\"&page={page_num}\"\n",
    "        image_urls = get_image_urls(page_url)\n",
    "        all_image_urls.extend(image_urls)\n",
    "        time.sleep(random.uniform(*DOWNLOAD_DELAY))\n",
    "    \n",
    "    if not all_image_urls:\n",
    "        print(f\"No images found for {model_name}!\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"Found {len(all_image_urls)} images for {model_name}\")\n",
    "    \n",
    "    # Download images\n",
    "    success_count = 0\n",
    "    model_folder = os.path.join(base_folder, model_name)\n",
    "    \n",
    "    for i, url in enumerate(tqdm(all_image_urls[:MAX_IMAGES_PER_MODEL], desc=f\"Downloading {model_name}\")):\n",
    "        try:\n",
    "            # Try to get higher quality version\n",
    "            high_quality_url = url.replace('webp_thumbnail', 'post')\n",
    "            response = requests.get(high_quality_url, headers=headers, timeout=15)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                file_ext = '.jpg' if 'jpg' in url else '.webp'\n",
    "                file_name = f\"{model_name}_{i+1}{file_ext}\"\n",
    "                with open(os.path.join(model_folder, file_name), 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                success_count += 1\n",
    "            else:\n",
    "                # Fallback to original URL\n",
    "                response = requests.get(url, headers=headers, timeout=15)\n",
    "                if response.status_code == 200:\n",
    "                    file_ext = '.jpg' if 'jpg' in url else '.webp'\n",
    "                    file_name = f\"{model_name}_{i+1}_thumb{file_ext}\"\n",
    "                    with open(os.path.join(model_folder, file_name), 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    success_count += 1\n",
    "            \n",
    "            time.sleep(random.uniform(*DOWNLOAD_DELAY))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError downloading image {i+1} for {model_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return success_count\n",
    "\n",
    "def generate_summary(base_folder):\n",
    "    \"\"\"Generate a summary report of downloaded images\"\"\"\n",
    "    print(\"\\nðŸ“Š Download Summary:\")\n",
    "    for model in target_models:\n",
    "        model_folder = os.path.join(base_folder, model)\n",
    "        if os.path.exists(model_folder):\n",
    "            count = len([f for f in os.listdir(model_folder) if f.endswith(('.jpg', '.webp'))])\n",
    "            print(f\"- {model}: {count} images\")\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸš€ Starting Divar car images scraper...\")\n",
    "    base_folder = create_folders()\n",
    "    \n",
    "    total_downloaded = 0\n",
    "    for model_name, query in target_models.items():\n",
    "        downloaded = scrape_model(model_name, query, base_folder)\n",
    "        total_downloaded += downloaded\n",
    "    \n",
    "    generate_summary(base_folder)\n",
    "    print(f\"\\nâœ… All operations completed! Total images downloaded: {total_downloaded}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848f708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
